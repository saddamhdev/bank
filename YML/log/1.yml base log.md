Excellent ğŸ‘
You want **full logging setup using only YAML (no XML)** â€” production-ready for **Banking Microservices + Splunk**.

Perfect. Letâ€™s build it cleanly.

---

# ğŸ“˜ Banking Microservices Logging (YAML-Based Setup)

---

# 1ï¸âƒ£ Goal

âœ” Separate log file per service
âœ” JSON structured logs
âœ” Correlation ID tracking
âœ” Log rotation
âœ” Splunk-ready format
âœ” No XML configuration

---

# 2ï¸âƒ£ Add Required Dependency (For JSON Logs)

In each service `pom.xml`:

```xml
<dependency>
    <groupId>net.logstash.logback</groupId>
    <artifactId>logstash-logback-encoder</artifactId>
    <version>7.4</version>
</dependency>
```

This enables JSON structured logging.

---

# 3ï¸âƒ£ application.yml Configuration (Full Banking Version)

Use this in **each service**:

```yaml
spring:
  application:
    name: gateway-service   # change per service

logging:
  level:
    root: INFO
    snvn: DEBUG

  file:
    name: logs/${spring.application.name}.log

  logback:
    rollingpolicy:
      max-file-size: 10MB
      max-history: 30

  pattern:
    console: >
      {"timestamp":"%d{yyyy-MM-dd'T'HH:mm:ss.SSS}",
       "level":"%level",
       "service":"${spring.application.name}",
       "thread":"%thread",
       "correlationId":"%X{correlationId}",
       "logger":"%logger",
       "message":"%msg"}%n

    file: >
      {"timestamp":"%d{yyyy-MM-dd'T'HH:mm:ss.SSS}",
       "level":"%level",
       "service":"${spring.application.name}",
       "thread":"%thread",
       "correlationId":"%X{correlationId}",
       "logger":"%logger",
       "message":"%msg"}%n
```

---

# 4ï¸âƒ£ What This Produces

Each service creates:

```text
logs/gateway-service.log
logs/user-service.log
logs/auth-service.log
...
```

Log Example (JSON):

```json
{
  "timestamp": "2026-02-21T14:15:22.334",
  "level": "INFO",
  "service": "user-service",
  "thread": "reactor-http-nio-2",
  "correlationId": "2fff823c",
  "logger": "snvn.service.UserService",
  "message": "User created successfully"
}
```

Splunk can automatically parse this.

---

# 5ï¸âƒ£ Correlation ID (Required For Distributed Banking)

## In Gateway GlobalFilter

Add:

```java
import org.slf4j.MDC;

String correlationId = UUID.randomUUID().toString();
MDC.put("correlationId", correlationId);
```

Before:

```java
return chain.filter(mutatedExchange);
```

After completion:

```java
.then(Mono.fromRunnable(() -> MDC.clear()));
```

---

# 6ï¸âƒ£ Log Storage Structure

On Windows (your case):

```
D:\module project\base\
   gateway-service\logs\
   user-service\logs\
   auth-service\logs\
```

---

# 7ï¸âƒ£ Splunk Setup (YAML-compatible logs)

Install Splunk Universal Forwarder
Then:

```bash
splunk add monitor "D:\module project\base"
```

Splunk will automatically ingest JSON logs.

---

# 8ï¸âƒ£ Splunk Queries (Banking Use Case)

## Trace One Request

```
correlationId="2fff823c"
```

## Errors Only

```
level=ERROR
```

## 5xx Errors

```
message="500"
```

## Service-wise Error Count

```
index=main level=ERROR | stats count by service
```

---

# 9ï¸âƒ£ Banking Best Practice

âœ” Separate log per service
âœ” JSON structured logs
âœ” Correlation ID everywhere
âœ” No password logging
âœ” 30-day retention
âœ” Splunk alert enabled

---

# ğŸ”Ÿ What Real Banks Do (Advanced)

Production usually adds:

* Separate security index
* Separate audit index
* Masked PII data
* Alert for fraud patterns
* Log retention policy (90+ days)
* Immutable audit logs

---

# ğŸ¯ Final Architecture

```text
Each Service â†’ YAML Logging â†’ Log File â†’ Splunk â†’ Dashboard â†’ Alert
```

No XML needed.

---

If you want next level now, I can show:

* ğŸ”¥ How to mask sensitive data automatically
* ğŸ”¥ How to create Splunk fraud detection dashboard
* ğŸ”¥ How to track one banking transaction across 10 services
* ğŸ”¥ Production Docker + Splunk setup
* ğŸ”¥ OpenTelemetry integration with Splunk

Tell me next step ğŸš€
